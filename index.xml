<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Aditi Asati</title><link>https://aditi-asati.github.io/</link><description>Recent content on Aditi Asati</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 06 Jun 2024 08:06:25 +0600</lastBuildDate><atom:link href="https://aditi-asati.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>How to Kernelize the Ridge Regression Algorithm</title><link>https://aditi-asati.github.io/posts/kernel_ridge_regression/</link><pubDate>Thu, 06 Jun 2024 08:06:25 +0600</pubDate><guid>https://aditi-asati.github.io/posts/kernel_ridge_regression/</guid><description>&lt;p>The ridge regression algorithm learns a linear function to map input points to a real number by minimizing an objective function. The optimization problem in ridge regression is given by:&lt;/p>
&lt;p>$$
\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^{n} \left( Y_i - \langle w, \Phi(X_i) \rangle \right)^2 + \lambda \lVert w \rVert_2^2$$&lt;/p>
&lt;p>Here, $\lambda$ denotes the tradeoff constant. The first term in the objective function is the training error (also called empirical risk of a predictor function) in terms of linear least squares error. The second term is the regularization term penalizing functions which have large L2-norm squared.&lt;/p></description></item></channel></rss>