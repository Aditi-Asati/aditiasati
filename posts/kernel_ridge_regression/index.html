<!doctype html><html lang=en><head><title>How to Kernelize the Ridge Regression Algorithm</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=/application.36e080bd5c38a8530af0f9a5ef6865a6b54d3c4ee19713271788ee3a70dce63d.css integrity="sha256-NuCAvVw4qFMK8Pml72hlprVNPE7hlxMnF4juOnDc5j0="><link rel=icon type=image/png href=/images/site/favicon_hu9036660780100762024.png><meta property="og:url" content="https://aditi-asati.github.io/posts/kernel_ridge_regression/"><meta property="og:site_name" content="Aditi Asati"><meta property="og:title" content="How to Kernelize the Ridge Regression Algorithm"><meta property="og:description" content="Markdown rendering samples"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-06-06T08:06:25+06:00"><meta property="article:modified_time" content="2024-06-06T08:06:25+06:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="How to Kernelize the Ridge Regression Algorithm"><meta name=twitter:description content="Markdown rendering samples"><meta name=description content="Markdown rendering samples"><script src=https://cdn.counter.dev/script.js data-id=2816408a-c813-467e-bcd4-a5d18672e483 data-utcoffset=1></script><script>theme=localStorage.getItem("darkmode:color-scheme")||"system",theme=="system"&&(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?theme="dark":theme="light"),document.documentElement.setAttribute("data-theme",theme)</script></head><body class="type-posts kind-page" data-bs-spy=scroll data-bs-target=#TableOfContents data-bs-offset=80><div class="container-fluid bg-secondary wrapper"><nav class="navbar navbar-expand-xl top-navbar shadow" id=top-navbar><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button>
<i data-feather=sidebar></i>
</button>
<a class=navbar-brand href=/><img src=/images/site/main-logo_hu11188698498150164404.png id=logo alt=Logo>
Aditi Asati</a>
<button class="navbar-toggler navbar-light" id=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#top-nav-items aria-label=menu>
<i data-feather=menu></i></button><div class="collapse navbar-collapse dynamic-navbar" id=top-nav-items><ul class="nav navbar-nav ms-auto"><li class=nav-item><a class=nav-link href=/#home>Home</a></li><li class=nav-item><a class=nav-link href=/#about>About</a></li><li class=nav-item><a class=nav-link href=/#skills>Skills</a></li><li class=nav-item><a class=nav-link href=/#experiences>Experiences</a></li><li class=nav-item><a class=nav-link href=/#education>Education</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>More</a><div class=dropdown-menu aria-labelledby=navbarDropdown><a class=dropdown-item href=/#projects>Projects</a>
<a class=dropdown-item href=/#recent-posts>Recent Posts</a>
<a class=dropdown-item href=/#accomplishments>Accomplishments</a></div></li><div id=top-navbar-divider></div><li class=nav-item><a class=nav-link id=blog-link href=/posts>Posts</a></li><li class=nav-item><a class=nav-link href=/files/cv.pdf>Resume</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg class=theme-icon src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme"></a><div id=themeMenu class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# data-scheme=light><img class=theme-icon src=/icons/sun-svgrepo-com.svg width=20 alt="Light Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=dark><img class=theme-icon src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=system><img class=theme-icon src=/icons/computer-svgrepo-com.svg width=20 alt="System Theme"></a></div></li></ul></div></div><img src=/images/site/main-logo_hu11188698498150164404.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu9411445442247943367.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts/ data-filter=all>Posts</a></li><div class=subtree><li><a class="active list-link" href=/posts/kernel_ridge_regression/ title="Kernelize Ridge Regression">Kernelize Ridge Regression</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ms-auto align-self-lg-center"><img class=rounded-circle src=/images/author/aditi_edited_3_hu7689788148758517732.jpg alt="Author Image"><h5 class=author-name>Aditi Asati</h5><p class=text-muted>Thursday, June 6, 2024 | 6 minutes</p></div><div class=title><h1>How to Kernelize the Ridge Regression Algorithm</h1></div><div class=tags><ul style=padding-left:0></ul></div><div class=post-content id=post-content><p>The ridge regression algorithm learns a linear function to map input points to a real number by minimizing an objective function. The optimization problem in ridge regression is given by:</p><p>$$
\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^{n} \left( Y_i - \langle w, \Phi(X_i) \rangle \right)^2 + \lambda \lVert w \rVert_2^2$$</p><p>Here, $\lambda$ denotes the tradeoff constant. The first term in the objective function is the training error (also called empirical risk of a predictor function) in terms of linear least squares error. The second term is the regularization term penalizing functions which have large L2-norm squared.</p><p>Linear methods like ridge regression work best when the underlying dataset is sort of linear. However, when we use linear methods to model a non-linear dataset, the machine learning model often tends to underfit the dataset as the model class is not strong enough to predict well on such a dataset.</p><p>Kernel methods were introduced to overcome this problem. Kernel methods allow us to implement linear methods on a non-linear dataset by implicitly embedding the input into a high-dimensional euclidean space in which the data-points become linear.</p><p>In this blog, we will learn how to kernelize the Ridge Regression algorithm and also see an implementation of it using <code>scikit-learn</code> library.</p><p>For we begin, let&rsquo;s familiarize ourselves with two terms which we will use later on.</p><ul><li>A <strong>kernel function</strong> takes two points in the input space $\mathcal{X}$ and maps them to a real number. Mathematically speaking, a kernel function is a function $k : \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ where $\mathbb{R}$ is the set of all real numbers. Intuitively, kernel function measures how ”similar” two points are in the feature space.</li></ul><p>Now that we have understood what kernel functions are, let us understand kernel matrix.</p><ul><li>Given a kernel function $k$ and a set of points $x_1, &mldr;, x_n \in \mathcal{X}$, the corresponding <strong>kernel matrix</strong> is defined as $K = (k(x_i, x_j))_{i,j \in n \times n}$. So each $ij$-th entry in the kernel matrix $K$ is the value of the kernel function at points $x_i$ and $x_j$.</li></ul><p>We are now ready to derive the kernel version of the Ridge Regression algorithm.</p><p>The idea is to rewrite the optimization problem of the Ridge Regression algorithm in terms of the kernel function or the kernel matrix. To this end, we will invoke the <em><strong>Representer theorem</strong></em> which basically states that for an optimization problem of the form:
$$ \min_{w \in \mathcal{H}} \left( R_n(w) + \lambda \Omega(|w|_{\mathcal{H}}) \right) $$</p><p>$\quad$ where:</p><ul><li>$R_n$ is the empirical risk (training error)</li><li>$\Omega$ is the regularizer function</li><li>$\lambda$ is the tradeff constant</li><li>$\mathcal{H}$ is the euclidean feature space</li><li>$w$ is a vector in $\mathcal{H}$</li></ul><div class="alert tip"><span><i data-feather=alert-circle></i></span>
<span><strong>$\mathcal{H}$ is actually the space of all real-valued functions from the input space $X$ to $\mathbb{R}$. It is infact a vector space.</strong></span></div><p>The solution exists and is given by:
$${w^* = \sum_{i=1}^{n} \alpha_i k(X_i, \cdot)}$$</p><p>$\quad$ where:</p><ul><li>$\alpha_i$ is a real number for all $i$</li><li>$k(X_i, .)$ is a function from the input space to $\mathbb{R}$.</li></ul><p>Notice that the optimization problem in Ridge Regression appears in the same form as mentioned in the Representer theorem.
Hence, we can apply the theorem and obtain a solution $w$ of the Ridge Regression problem as a linear combination of input points in the feature space:</p><p>$$w = \sum_{j=1}^n \alpha_j\Phi(X_j)$$</p><p>Now this $w$ can be substituted back into the above ridge regression problem yielding the following optimization problem in terms of the kernel matrix $K$:</p><p>$$\min_{\alpha \in \mathbb{R}^n} \frac{1}{n} \lVert Y - K\alpha \rVert_2^2 + \lambda \alpha^T K \alpha$$<div style=margin-top:3rem></div></p><p>The solution to this optimization problem can be derived analytically. Let&rsquo;s take a look at the derivation:</p><div style=margin-top:3rem></div><p><strong>Step 1</strong>:
The objective function in the above optimization problem is</p><p>$$Obj(\alpha) := \frac{1}{n} \lVert Y - K\alpha \rVert_2^2 + \lambda \alpha^T K \alpha$$</p><p>It is a convex function, therefore, any local minima is already a global minima of the function. Hence, it is enough to find one local minima of the objective function.</p><p><strong>Step 2</strong>:
In order to find a local minima, we will take the derivative of the objective function with respect to $\alpha$ and set it to 0:</p><p>$$grad(Obj)(\alpha) = -\frac{1}{n}K^T(y-K\alpha) + \lambda K\alpha + 0$$</p><p>(Note that $K$ is symmetric)</p><p>$$K(-y + K\alpha + n\lambda \alpha) = 0$$</p><p>This implies
$$\alpha = (K + n\lambda I)^{-1}Y.$$</p><p>And we have obtained the solution of the Kernel Ridge Regression problem!!</p><p>Now lets also formulate the evaluation function $f(x)$ in terms of the kernel function, after all the essence of kernelizing an algorithm is to write the optimization problem and the evaluation/ target function in terms of the kernel function/ kernel matrix.</p><p>$$f(x) = \langle w, \Phi(x) \rangle = \langle w, k(x, .) \rangle$$</p><div class="alert info"><span><i data-feather=info></i></span>
<span><strong>Note that $\Phi(x) = k_x = k(x, .)$.</strong></span></div><p>Substituting the value of w (given by the representer theorem) into the above equation yields the following:</p><p>$$f(x) = \langle \sum_{i=1}^{n} \alpha_i k(X_i, \cdot), k(x,.) \rangle$$</p><p>$$f(x) =\sum_{i=1}^n \alpha_i \langle k(X_i, .),k(x,.) \rangle = \sum_{i=1}^n \alpha_i \langle \Phi(X_i),\Phi(x) \rangle$$</p><div class="alert info"><span><i data-feather=info></i></span>
<span><strong>Note that $k(x,y) = \langle \Phi(x), \Phi(y) \rangle$. For more details regarding it, read <a href="https://drive.google.com/file/d/1QbEQNjbfIPkVEe-qEvwFPEyLx-tg255r/view?usp=sharing" target=_blank rel=noopener>here</a>.</strong></span></div><p>$$f(x) = \sum_{i=1}^n\alpha_ik(X_i, x)$$</p><p>We have finally obtained the target function for the kernel ridge regression algorithm. Observe that the function depends only on the kernel and not on the input points explicitly as discussed earlier.</p><p>Now, lets look at the implementation of the Kernel Ridge Regression algorithm using <code>scikit-learn</code> library.</p><p>We will begin by installing necessary dependencies.
Run the following command on the terminal.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>pip install scikit<span style=color:#f92672>-</span>learn
</span></span></code></pre></div><div style=margin-top:3rem></div><p>For demonstration, we will use the diabetes toy dataset present in the <code>scikit-learn</code> library.</p><p>Lets now import the required utilities.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.datasets <span style=color:#f92672>import</span> load_diabetes
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.kernel_ridge <span style=color:#f92672>import</span> KernelRidge
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.model_selection <span style=color:#f92672>import</span> train_test_split, GridSearchCV
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> mean_absolute_error, mean_squared_error
</span></span></code></pre></div><div style=margin-top:3rem></div><p>We will now load the diabetes dataset and extract the <code>data</code> matrix and the <code>target</code> array.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>diabetes <span style=color:#f92672>=</span> load_diabetes()
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> diabetes<span style=color:#f92672>.</span>data
</span></span><span style=display:flex><span>target <span style=color:#f92672>=</span> diabetes<span style=color:#f92672>.</span>target
</span></span></code></pre></div><div style=margin-top:3rem></div><p>Its time to split the dataset into train and test sets in order to evaluate the generalization performance of the Kernel Ridge Regression (KRR) model and also to avoid overfitting.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>X_train, X_test, y_train, y_test <span style=color:#f92672>=</span> train_test_split(data, target, test_size<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>)
</span></span></code></pre></div><p><div style=margin-top:2rem></div><div class="alert info"><span><i data-feather=info></i></span>
<span><strong>It&rsquo;s common practice to standardize the training and testing features (<code>X_train</code> and <code>X_test</code>) before training a model. However, we omit this step here because the feature matrix <code>data</code> provided by the <code>load_diabetes()</code> function is already standardized.</strong></span></div></p><div style=margin-top:3rem></div><p>We will be tuning the hyperparameters of the KRR model using Gridsearch cross validation method.</p><p><div style=margin-top:2rem></div><div class="alert info"><span><i data-feather=info></i></span>
<span><strong>It&rsquo;s crucial to always tune the hyperparameters of any machine learning model while training, since the performance of the model is very sensitive to the choice of the hyperparameters. As an example, think of how changing the regularization constant in KRR algorithm can affect its training.</strong></span></div></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>krr_model <span style=color:#f92672>=</span> KernelRidge()
</span></span><span style=display:flex><span>param_grid <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;alpha&#34;</span>: [<span style=color:#ae81ff>1e-5</span>, <span style=color:#ae81ff>0.00001</span>, <span style=color:#ae81ff>0.0001</span>, <span style=color:#ae81ff>0.001</span>, <span style=color:#ae81ff>0.01</span>, <span style=color:#ae81ff>0.1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>], <span style=color:#e6db74>&#34;kernel&#34;</span>: [<span style=color:#e6db74>&#34;linear&#34;</span>, <span style=color:#e6db74>&#34;rbf&#34;</span>, <span style=color:#e6db74>&#34;poly&#34;</span>, <span style=color:#e6db74>&#34;sigmoid&#34;</span>, ]}
</span></span><span style=display:flex><span>grid_search <span style=color:#f92672>=</span> GridSearchCV(krr_model, param_grid, scoring<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;neg_mean_absolute_error&#34;</span>, n_jobs<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>, cv<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>)
</span></span></code></pre></div><p><div style=margin-top:2rem></div><div class="alert info"><span><i data-feather=info></i></span>
<span><strong>To understand Grid search cross validation, read <a href=https://towardsdatascience.com/cross-validation-and-grid-search-efa64b127c1b target=_blank rel=noopener>here</a>.</strong></span></div></p><div style=margin-top:3rem></div><p>Lets train our KRR model on the diabetes dataset and get the best hyperparameter values along with the trained model.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>grid_search<span style=color:#f92672>.</span>fit(X_train, y_train)
</span></span><span style=display:flex><span>best_params <span style=color:#f92672>=</span> grid_search<span style=color:#f92672>.</span>best_params_
</span></span><span style=display:flex><span>print(best_params)
</span></span><span style=display:flex><span>best_model <span style=color:#f92672>=</span> grid_search<span style=color:#f92672>.</span>best_estimator_
</span></span></code></pre></div><div style=margin-top:3rem></div><p>Now that we have trained our KRR model, its time to make predictions on the test set and compute the performance metrics on both the training and test sets.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span>predictions <span style=color:#f92672>=</span> best_model<span style=color:#f92672>.</span>predict(X_test)
</span></span><span style=display:flex><span>test_mae <span style=color:#f92672>=</span> mean_absolute_error(y_test, predictions)
</span></span><span style=display:flex><span>test_mse <span style=color:#f92672>=</span> mean_squared_error(y_test, predictions)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_predictions <span style=color:#f92672>=</span> best_model<span style=color:#f92672>.</span>predict(X_train)
</span></span><span style=display:flex><span>train_mae <span style=color:#f92672>=</span> mean_absolute_error(y_train, train_predictions)
</span></span><span style=display:flex><span>train_mse <span style=color:#f92672>=</span> mean_squared_error(y_train, train_predictions)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Test MAE : </span><span style=color:#e6db74>{</span>test_mae<span style=color:#e6db74>}</span><span style=color:#e6db74> and Test MSE : </span><span style=color:#e6db74>{</span>test_mse<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Train MAE : </span><span style=color:#e6db74>{</span>train_mae<span style=color:#e6db74>}</span><span style=color:#e6db74> and train MSE : </span><span style=color:#e6db74>{</span>train_mse<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><div style=margin-top:3rem></div><p>Here&rsquo;s the full code:
<script src=https://gist.github.com/Aditi-Asati/996d0cd86b7fd15911cefe44a608c225.js></script></p><p>Woohoo! We have come a long way. I hope you found this blog helpful.
Thanks for reading!</p></div><div class="row ps-3 pe-3"><div class="col-md-6 share-buttons"><strong>Share on:</strong>
<a class="btn icon-button bg-facebook" href="https://www.facebook.com/sharer.php?u=https%3a%2f%2faditi-asati.github.io%2fposts%2fkernel_ridge_regression%2f" target=_blank><i class="fab fa-facebook"></i>
</a><a class="btn icon-button bg-twitter" href="https://twitter.com/share?url=https%3a%2f%2faditi-asati.github.io%2fposts%2fkernel_ridge_regression%2f&text=How%20to%20Kernelize%20the%20Ridge%20Regression%20Algorithm&via=Aditi%20Asati" target=_blank><i class="fab fa-twitter"></i>
</a><a class="btn icon-button bg-reddit" href="https://reddit.com/submit?url=https%3a%2f%2faditi-asati.github.io%2fposts%2fkernel_ridge_regression%2f&title=How%20to%20Kernelize%20the%20Ridge%20Regression%20Algorithm" target=_blank><i class="fab fa-reddit"></i>
</a><a class="btn icon-button bg-linkedin" href="https://www.linkedin.com/shareArticle?url=https%3a%2f%2faditi-asati.github.io%2fposts%2fkernel_ridge_regression%2f&title=How%20to%20Kernelize%20the%20Ridge%20Regression%20Algorithm" target=_blank><i class="fab fa-linkedin"></i>
</a><a class="btn icon-button bg-whatsapp" href="https://api.whatsapp.com/send?text=How%20to%20Kernelize%20the%20Ridge%20Regression%20Algorithm https%3a%2f%2faditi-asati.github.io%2fposts%2fkernel_ridge_regression%2f" target=_blank><i class="fab fa-whatsapp"></i>
</a><a class="btn icon-button" href="mailto:?subject=How%20to%20Kernelize%20the%20Ridge%20Regression%20Algorithm&body=https%3a%2f%2faditi-asati.github.io%2fposts%2fkernel_ridge_regression%2f" target=_blank><i class="fas fa-envelope-open-text"></i></a></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/aditi-asati/aditi-asati.github.io/edit/main/content/posts/kernel_ridge_regression/index.md title="Improve this page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>
Improve this page</a></div></div><hr><div class="row next-prev-navigator"></div><hr><div id=disqus_thread></div><script type=text/javascript>(function(){if(window.location.hostname=="localhost")return;var t,e=document.createElement("script");e.type="text/javascript",e.async=!0,t="toha-example-site",e.src="//"+t+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)})()</script><noscript>Please enable JavaScript to view the
<a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com/ class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div></div><a id=scroll-to-top class=btn type=button data-bs-toggle=tooltip data-bs-placement=left title="Scroll to top"><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center ps-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-start"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://aditi-asati.github.io/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://aditi-asati.github.io/#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=https://aditi-asati.github.io/#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=https://aditi-asati.github.io/#education>Education</a></li><li class=nav-item><a class=smooth-scroll href=https://aditi-asati.github.io/#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=https://aditi-asati.github.io/#recent-posts>Recent Posts</a></li><li class=nav-item><a class=smooth-scroll href=https://aditi-asati.github.io/#accomplishments>Accomplishments</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:scholar.aditiasati@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>scholar.aditiasati@gmail.com</span></a></li><li><a href=https://github.com/Aditi-Asati target=_blank rel=noopener><span><i class="fab fa-github"></i></span> <span>Aditi-Asati</span></a></li><li><a href=https://www.linkedin.com/in/aditi-asati-514a36253 target=_blank rel=noopener><span><i class="fab fa-linkedin"></i></span> <span>Aditi Asati</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-start"><div class=col-md-4><a id=theme href=https://github.com/hugo-toha/toha target=_blank rel=noopener><img src=/images/theme-logo_hu16779671404603505019.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2024 Copyright.</div><div class="col-md-4 text-end"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/application.100f2124055918d37ed55689f8b8cd9378a4ece97b585240e75bfd4894e0fe97.js integrity="sha256-EA8hJAVZGNN+1VaJ+LjNk3ik7Ol7WFJA51v9SJTg/pc=" defer></script></body></html>